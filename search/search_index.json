{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Foundation - L400 Workshop","text":"<p>Customers travel choices can be extremely nuanced as they seek out types of experiences. In this lab, Intelligent Flight planner chatbot supports customer request to modify upcoming flight. </p>"},{"location":"#lab-summary","title":"Lab Summary","text":"<ul> <li>Flight schedule (PDF) is document processed , standardized (Data Quality checked) and loaded to vector store using batch method \u2013 Services used AWS Textract, AWS Comprehend, AWS Glue, AWS Glue Data Quality, Opensearch</li> <li>Real-time flight schedule updates are made available in knowledge store using streaming ETL \u2013 Services used \u2013 Amazon MSK, Amazon Data firehose, AWS Opensearch</li> <li>Processed data is  catalogued using Amazon Datazone</li> <li>Building genAI using Bedrock and Q</li> <li>User prompt are stored (prompt engineered using streaming ETL) into chat history using streaming ETL and chatbot make  flight suggestions using AWS Bedrock inference</li> </ul>"},{"location":"setup/","title":"Data Foundation - L400 Workshop - Set Up","text":""},{"location":"cfn_setup/overview/exercise/","title":"Overview","text":""},{"location":"cfn_setup/overview/exercise/#step-1-deploy-a-cloudformation-template","title":"Step 1: Deploy a CloudFormation Template","text":"<p>In this lab, you will execute a ready-to-go CloudFormation template that does 3 things:</p> <ul> <li>Creates a storage bucket where you will store your files, using Amazon Simple Storage Service (Amazon S3)</li> <li>Deploys a compute function\u2014called a Lambda function\u2014that triggers AWS services to act on files when they are delivered to your S3 bucket. These services will transform the information in the document into data and convert unstructured text to relevant health standards. <li>Create an Identity and Access Management (IAM) role for your Lambda function that allows it to call those AWS services.</li>"},{"location":"cfn_setup/overview/exercise/#what-is-cloudformation","title":"What is CloudFormation?","text":"<p>AWS CloudFormation is a service that provides an easy way to create a collection of AWS and third-party resources, and provision and manage them in an orderly and predictable fashion. With an AWS CloudFormation template, you don\u2019t have to deploy each service independently. Instead, the CloudFormation template includes all the code to deploy the specific resources you need to complete this task.</p> <p>To launch the CloudFormation stack, first you need to download the CloudFormation template using this Download Template link."},{"location":"cfn_setup/overview/exercise/#deploy-a-cloudformation-template","title":"Deploy a CloudFormation template","text":"<p>In this section, you will deploy a CloudFormation template. When you execute this CloudFormation template, a set of prescribed AWS services are deployed in your account. The outcome of the CloudFormation template used in this section is the creation of two S3 buckets to store files and two Lambda functions with appropriate role access permissions to trigger selected AWS services.</p> <ol> <li>Go to CloudFormation service by using this link\u00a0.</li> <li>Under Prerequisite - Prepare template section:<ul> <li>For Prepare template, select Template is ready.</li> </ul> </li> <li>Under Specify template section:<ul> <li>For Template source, select Upload a template file.</li> <li>For Upload a template file, click Choose file, and select the file that you previously downloaded at the beginning of this section. </li> </ul> </li> <li>Click Next.</li> <li>Under the Specify stack details section, enter stack name as <code>workshop</code>, then click Next. </li> <li>Retain all selected default values on the Configure stack options screen, scroll down, then click Next. </li> <li>In the Review screen, select the checkbox for I acknowledge that AWS CloudFormation might create IAM resources., then click Submit. </li> <li>The CloudFormation stack creation will roughly take 5-10 minutes to complete. Once the stack deployment is complete, you will see a status of \"CREATE_COMPLETE\". </li> </ol>"},{"location":"cfn_setup/overview/exercise/#services-that-are-deployed-using-the-cloudformation-template","title":"Services that are deployed using the CloudFormation template","text":"<p>Once your CloudFormation deployment completes, the following resources will exist in your AWS environment:</p> <p></p> <ul> <li> <p>Two S3 buckets to store the different phases of your analyses:</p> <ul> <li>Document Upload Bucket: the storage location to which you\u2019ll upload your document files. Objects saved in this S3 bucket will trigger a Lambda function to call the Amazon Textract service which extracts typed information from those document files.</li> <li>Textract Results Bucket: the storage location where the text files containing the text extracted from your document files is stored. Files saved in this bucket will trigger a Lambda function to call the Amazon Comprehend Medical service that can process unstructured text to detect health standards entities.</li> </ul> </li> <li> <p>One  Lambda functions, assigned the correct roles to perform their tasks:</p> <ul> <li>Process Document Function: This Lambda function gets invoked when a document file is successfully uploaded to your Document Upload Bucket. The CloudFormation template establishes a configuration in the Document Upload Bucket\u2019s S3 Event Notifications to notify this Lambda function that a document file was uploaded. Upon receipt of this notification, this Lambda function extracts text from the document file by calling the Amazon Textract service via API. This Lambda function then writes the extracted information to the Textract Results Bucket as a text file.</li> </ul> </li> </ul>"},{"location":"cfn_setup/overview/exercise/#services-overview","title":"Services Overview","text":"<p>Here is a brief overview of AWS services and concepts deployed by the CloudFormation template:</p> <ul> <li>S3: Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.</li> <li>IAM role: An AWS Identity and Access Management (IAM) role is an IAM identity that you can create in your account with specific permissions. An IAM role is similar to an IAM user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. Instead of being uniquely associated with one person, a role can be assumed by anyone who needs it. Also, a role does not have standard long-term credentials such as a password or access keys associated with it. When you assume a role, it provides you with temporary security credentials for your role session.</li> <li>Lambda: AWS Lambda is a compute service that lets you run code without provisioning or managing servers. With Lambda, all you need to do is supply your code in one of the language run-times that Lambda supports. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, and logging. The Lambda service runs your code\u2014organized as Lambda functions\u2014only when needed and scales automatically. You only pay for the compute time that you consume\u2014there is no charge when your code is not running.</li> <li>Lambda execution role: A Lambda function's execution role is an IAM role that grants the function permission to access AWS services and resources. For example, you might create an execution role with permission to send logs to Amazon CloudWatch service that allows you to collect and visualize those logs.</li> <li>S3 Event Notifications: You can set up the Amazon S3 Event Notifications feature to send notifications when selected events happen in your S3 bucket. With this feature, you identify the destination for each notification. One possible notification destination is an AWS Lambda Function, which would allow you to invoke a Lambda function to perform file processing tasks.</li> </ul> <p>Congratulations! You successfully deployed a CloudFormation template, and with it, you set up three S3 buckets, two Lambda functions, and the appropriate IAM roles for those Lambda functions!.</p>"},{"location":"governance/overview/","title":"Place holder for Data Governance","text":""},{"location":"preparerag/kbase/","title":"Preparing knowledge base for RAG","text":""},{"location":"preparerag/kbase/#how-it-works","title":"How it works","text":"<p>Knowledge base for Amazon Bedrock help you take advantage of Retrieval Augmented Generation (RAG), a popular technique that involves drawing information from a data store to augment the responses generated by Large Language Models (LLMs). When you set up a knowledge base with your data sources, your application can query the knowledge base to return information to answer the query either with direct quotations from sources or with natural responses generated from the query results.</p> <p>With knowledge bases, you can build applications that are enriched by the context that is received from querying a knowledge base. It enables a faster time to market by abstracting from the heavy lifting of building pipelines and providing you an out-of-the-box RAG solution to reduce the build time for your application. Adding a knowledge base also increases cost-effectiveness by removing the need to continually train your model to be able to leverage your private data.</p> <p>The following diagrams illustrate schematically how RAG is carried out. Knowledge base simplifies the setup and implementation of RAG by automating several steps in this process.</p>"},{"location":"preparerag/kbase/#pre-processing-data","title":"Pre-processing data","text":"<p>To enable effective retrieval from private data, a common practice is to first split the documents into manageable chunks for efficient retrieval. The chunks are then converted to embeddings and written to a vector index, while maintaining a mapping to the original document. These embeddings are used to determine semantic similarity between queries and text from the data sources. The following image illustrates pre-processing of data for the vector database.</p> <p></p>"},{"location":"preparerag/kbase/#runtime-execution","title":"Runtime execution","text":"<p>At runtime, an embedding model is used to convert the user's query to a vector. The vector index is then queried to find chunks that are semantically similar to the user's query by comparing document vectors to the user query vector. In the final step, the user prompt is augmented with the additional context from the chunks that are retrieved from the vector index. The prompt alongside the additional context is then sent to the model to generate a response for the user. The following image illustrates how RAG operates at runtime to augment responses to user queries.</p> <p></p>"},{"location":"preparerag/overview/","title":"Data Foundations","text":"<p>These are the core elements of a modern, robust data foundation.</p> <ul> <li> <p>For genAI you need to store various types of data including unstructured, structured, streaming, and vector data that can be used for building and fine-tuning models as well as adding context to prompts through RAG. AWS provides the industry\u2019s most comprehensive set of data services for storing and querying any type of data at scale.</p> </li> <li> <p>You\u2019ll also want to make sure that you have ready access to all of your data to leverage for genAI applications. AWS is integrating services our services together and integrating with other data sources so you can break down data silos and get a complete picture of your business or customer.  \u00a0</p> </li> <li>Finally, Data needs to be secure and governed throughout the lifecycle of building a genAI application. AWS offers several tools to help ensure data quality, privacy, and access controls so the data you use for genAI applications is high-quality and compliant.</li> </ul>"},{"location":"preparerag/overview/#data-foundation-for-genai-high-level-architecture","title":"Data Foundation for GenAI - High level architecture","text":"<p>One of the very first things that gen AI introduces to the pipeline is the need for additional data sources. And these are primarily in the form of unstructured data. Now, unstructured data is not a new construct to the data world, however it is now the dominant part of the data feed for a gen AI pipeline. </p> <p>Our typical data architecture patterns have largely dealt with structured data, but with more than 80% of the world\u2019s data in unstructured formats, it is now front and center with gen AI, and has a big influence on data ingestion, data integration and data governance.  Unstructured data\u00a0is information that doesn\u2019t conform to a predefined schema or isn\u2019t organized according to a preset data model.\u00a0Understanding the data, categorizing it, and extracting insights from it can be challenging. Metadata discovery is one of the primary requirements in data analytics.</p>"},{"location":"preparerag/rag/","title":"What is Retrieval-Augmented Generation?","text":"<p>Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.</p>"},{"location":"preparerag/rag/#understanding-a-retrieval-augmented-generation-pattern","title":"Understanding  a Retrieval-Augmented Generation Pattern","text":""},{"location":"streamprocess/kds/","title":"Create a Kinesis Data Stream","text":"<p>In this section, you will create a new Kinesis Data Stream , this data stream handles incoming real-time flight schedule updates.</p> <ol> <li> <p>Using the AWS Console, navigate to the Amazon Kinesis services and press Get Started when prompted.</p> </li> <li> <p>Select Create data stream to navigate to the Amazon Kinesis Data Stream service. </p> </li> <li> <p>When prompted, enter input-stream as your Kinesis Data Stream name. Keep 'On demand' selected as your capacity mode, and leave all else as default. </p> </li> </ol> <pre><code>We will use on-demand for this workshop for simplicity. As discussed in the previous section, on-demand mode takes away the guesswork of shard estimation, and provisions capacity based on your throughput. Use provisioned mode if you have a predictable throughput for cost optimization.\n</code></pre> <p>Check out when to use on-demand vs provisioned</p> <ol> <li>When finally created, you should see a status of Active on the stream. </li> </ol>"},{"location":"streamprocess/overview/","title":"Stream Processing Overview","text":"<p>Batch ingestion usually relies on data aggregated over a period of time. Sometimes you need to receive data in real-time, or as close to real-time as possible. Streaming data are generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, information from social networks, geospatial services, Internet of Things (IoT) devices that measure air or water quality (or other environmental factors), or telemetry from personal monitoring or fitness devices. </p> <p>In this lab scenario,  flight schedule changes and real time updates are made to data lake.</p>"},{"location":"streamprocess/overview/#architecture","title":"Architecture","text":""},{"location":"streamprocess/overview/#kinesis-data-streams","title":"Kinesis Data Streams","text":"<p>What is KDS?</p> <p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website click streams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>"},{"location":"streamprocess/write_data_to_kds/","title":"Write Data to a Kinesis Data Stream using Amazon Managed Service for Apache Flink Studio Notebook","text":"<p>In this section, we will dive into the concepts of the Studio Notebooks for Managed Service for Apache Flink</p> <p>Studio notebooks for Managed Service for Apache Flink allows you to interactively query data streams in real time, and easily build and run stream processing applications using standard SQL, Python, and Scala. With a few clicks in the AWS Management console, you can launch a serverless notebook to query data streams and get results in seconds.</p> <p>A notebook is a web-based development environment. With notebooks, you get a simple interactive development experience combined with the advanced capabilities provided by Apache Flink. Studio notebooks uses notebooks powered by Apache Zeppelin, and uses Apache Flink as the stream processing engine. Studio notebooks seamlessly combines these technologies to make advanced analytics on data streams accessible to developers of all skill sets.</p> <p>Apache Zeppelin provides your Studio notebooks with a complete suite of analytics tools, including the following:</p> <ol> <li>Data Visualization</li> <li>Exporting data to files</li> <li>Controlling the output format for easier analysis</li> </ol> <p>With a notebook, you model queries using the Apache Flink Table API &amp; SQL in SQL, Python, or Scala, or DataStream API in Scala. With a few clicks, you can then promote the Studio notebook to a continuously-running, non-interactive, Managed Service for Apache Flink stream-processing application for your production workloads.</p> <p>Let's now start working on our lab.</p>"}]}